{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Metrics Hub - Complete Demo Notebook\n",
    "\n",
    "> **Interactive demonstration of Metrics Hub package from installation to deployment**\n",
    "\n",
    "This notebook provides a complete walkthrough of Metrics Hub capabilities including:\n",
    "- 📦 Local package installation and verification\n",
    "- 🔧 Metric creation and implementation\n",
    "- 🧪 Testing with rich outputs (DataFrames, Plotly charts)\n",
    "- 🗑️ Metric cleanup and removal\n",
    "- 📊 Interactive dashboard demonstration\n",
    "- 🌐 API server usage and testing\n",
    "- 🚀 Deployment preparation for Posit Connect\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand how to:\n",
    "- Install and set up Metrics Hub from source\n",
    "- Create metrics with rich outputs (DataFrames, Plotly charts, mixed objects)\n",
    "- Test metrics interactively using the built-in dashboard\n",
    "- Remove metrics and clean up files when no longer needed\n",
    "- Use the API server for production deployment\n",
    "- Prepare metrics for Posit Connect deployment\n",
    "\n",
    "## ⚡ Quick Start Options\n",
    "\n",
    "### 🧪 **Option 1: Run This Notebook (Recommended)**\n",
    "- Follow all cells step-by-step for complete experience\n",
    "- See live metric execution and rich outputs\n",
    "- Interactive exploration of all features\n",
    "\n",
    "### 🚀 **Option 2: Try Components Directly**\n",
    "\n",
    "```bash\n",
    "# Test installation first\n",
    "python test_installation.py\n",
    "\n",
    "# List available metrics  \n",
    "metrics-hub list\n",
    "\n",
    "# Create a new metric\n",
    "metrics-hub create \"Your Metric\" --template simple\n",
    "\n",
    "# Remove a metric\n",
    "metrics-hub remove your_metric_id\n",
    "\n",
    "# Try the interactive dashboard\n",
    "metrics-hub dashboard\n",
    "# Open: http://localhost:8050\n",
    "\n",
    "# Try the API server\n",
    "metrics-hub api  \n",
    "# Open: http://localhost:8000/docs\n",
    "```\n",
    "\n",
    "## 📋 Notebook Contents Overview\n",
    "\n",
    "| Step | Description | Key Features |\n",
    "|------|-------------|--------------|\n",
    "| 1-2 | **Installation & Setup** | Package verification, structure exploration |\n",
    "| 3-5 | **Example Metrics** | Test built-in financial analysis with rich outputs |\n",
    "| 6-8 | **Create New Metric** | Scaffolding system, template enhancement |\n",
    "| 9 | **Metric Cleanup** | Remove command, file cleanup |\n",
    "| 10-12 | **API Integration** | FastAPI server, multiple output formats |\n",
    "| 13-14 | **Dashboard & Deployment** | Interactive testing, Posit Connect prep |\n",
    "| 15 | **Advanced Features** | Output formats, testing framework |\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Let's start building powerful metrics with rich outputs! 🎉**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Step 1: Installation\n",
    "\n",
    "⚠️ **IMPORTANT**: The package has been **fixed and tested** to work correctly! Any previous import errors have been resolved.\n",
    "\n",
    "✅ **All components are working**:\n",
    "- Metric registry and discovery\n",
    "- Example financial analysis metric  \n",
    "- CLI commands (`list`, `create`, `dashboard`, `api`)\n",
    "- API server and dashboard applications\n",
    "- Rich output processing (DataFrames, Plotly charts)\n",
    "\n",
    "Let's install Metrics Hub from the local development version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/alan/Documents/GitHub/metric\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (8.2.1)\n",
      "Requirement already satisfied: fastapi>=0.100.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (0.116.1)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: plotly>=5.14.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (6.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (6.0.1)\n",
      "Requirement already satisfied: uvicorn>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (0.35.0)\n",
      "Requirement already satisfied: black>=23.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (25.1.0)\n",
      "Requirement already satisfied: dash>=2.14.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (3.1.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (5.10.4)\n",
      "Requirement already satisfied: psycopg2-binary>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (2.9.10)\n",
      "Requirement already satisfied: pytest-asyncio>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (1.1.0)\n",
      "Requirement already satisfied: pytest>=7.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (8.4.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (2.32.4)\n",
      "Requirement already satisfied: ruff>=0.0.270 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (0.12.8)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from metrics-hub==1.0.0) (2.0.41)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from black>=23.3.0->metrics-hub==1.0.0) (1.1.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from black>=23.3.0->metrics-hub==1.0.0) (25.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from black>=23.3.0->metrics-hub==1.0.0) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/alan/Library/Python/3.12/lib/python/site-packages (from black>=23.3.0->metrics-hub==1.0.0) (4.2.0)\n",
      "Requirement already satisfied: Flask<3.2,>=1.0.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (3.1.1)\n",
      "Requirement already satisfied: Werkzeug<3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (8.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (4.14.1)\n",
      "Requirement already satisfied: retrying in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (1.4.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/alan/Library/Python/3.12/lib/python/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dash>=2.14.0->metrics-hub==1.0.0) (80.9.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fastapi>=0.100.0->metrics-hub==1.0.0) (0.47.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nbformat>=4.2.0->metrics-hub==1.0.0) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nbformat>=4.2.0->metrics-hub==1.0.0) (4.25.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/alan/Library/Python/3.12/lib/python/site-packages (from nbformat>=4.2.0->metrics-hub==1.0.0) (5.7.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/alan/Library/Python/3.12/lib/python/site-packages (from nbformat>=4.2.0->metrics-hub==1.0.0) (5.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=2.0.0->metrics-hub==1.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=2.0.0->metrics-hub==1.0.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=2.0.0->metrics-hub==1.0.0) (2024.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from plotly>=5.14.0->metrics-hub==1.0.0) (1.46.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.0.0->metrics-hub==1.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.0.0->metrics-hub==1.0.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.0.0->metrics-hub==1.0.0) (0.4.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytest>=7.3.0->metrics-hub==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytest>=7.3.0->metrics-hub==1.0.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /Users/alan/Library/Python/3.12/lib/python/site-packages (from pytest>=7.3.0->metrics-hub==1.0.0) (2.17.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->metrics-hub==1.0.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->metrics-hub==1.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->metrics-hub==1.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->metrics-hub==1.0.0) (2025.7.9)\n",
      "Requirement already satisfied: h11>=0.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from uvicorn>=0.22.0->metrics-hub==1.0.0) (0.16.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask<3.2,>=1.0.4->dash>=2.14.0->metrics-hub==1.0.0) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask<3.2,>=1.0.4->dash>=2.14.0->metrics-hub==1.0.0) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask<3.2,>=1.0.4->dash>=2.14.0->metrics-hub==1.0.0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask<3.2,>=1.0.4->dash>=2.14.0->metrics-hub==1.0.0) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->metrics-hub==1.0.0) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->metrics-hub==1.0.0) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->metrics-hub==1.0.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->metrics-hub==1.0.0) (0.26.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->metrics-hub==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.100.0->metrics-hub==1.0.0) (4.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from importlib-metadata->dash>=2.14.0->metrics-hub==1.0.0) (3.23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.100.0->metrics-hub==1.0.0) (1.3.1)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: metrics-hub\n",
      "  Building editable for metrics-hub (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for metrics-hub: filename=metrics_hub-1.0.0-py3-none-any.whl size=6709 sha256=820cf5fa7939ae3a866c2b07f8d738247b70898816f5c592c0c2a727730faab3\n",
      "  Stored in directory: /private/var/folders/h8/23250j7n7mq4rh4gnm_4v8d80000gn/T/pip-ephem-wheel-cache-hujj5_3t/wheels/cf/b8/eb/81ad5a6a808d174602c8f580479484c0f232b32250e62304a2\n",
      "Successfully built metrics-hub\n",
      "Installing collected packages: metrics-hub\n",
      "  Attempting uninstall: metrics-hub\n",
      "    Found existing installation: metrics-hub 1.0.0\n",
      "    Uninstalling metrics-hub-1.0.0:\n",
      "      Successfully uninstalled metrics-hub-1.0.0\n",
      "Successfully installed metrics-hub-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "zsh:1: 4.2.0 not found\n",
      "🔍 Running installation verification test...\n",
      "🚀 Metrics Hub Installation Test\n",
      "========================================\n",
      "🔍 Testing imports...\n",
      "  ✅ metrics_hub imported successfully (version: 1.0.0)\n",
      "  ✅ Core functions imported\n",
      "  ✅ API components imported\n",
      "  ✅ Dashboard components imported\n",
      "  ✅ Registry components imported\n",
      "\n",
      "📊 Testing metric registry...\n",
      "  ✅ Registry loaded successfully\n",
      "  📊 Found 1 metrics:\n",
      "    • example_financial_analysis: Example Financial Analysis\n",
      "\n",
      "🧪 Testing metric calculation...\n",
      "  🧮 Testing metric: example_financial_analysis\n",
      "  ✅ Metric calculation successful!\n",
      "  📊 Result keys: ['price_data', 'performance_chart', 'correlation_heatmap', 'statistics_table', 'summary_metrics']\n",
      "    price_data: DataFrame\n",
      "    performance_chart: Figure\n",
      "    correlation_heatmap: Figure\n",
      "    statistics_table: DataFrame\n",
      "    summary_metrics: dict\n",
      "\n",
      "🌐 Testing API creation...\n",
      "  ✅ API app created successfully\n",
      "\n",
      "📊 Testing dashboard creation...\n",
      "  ✅ Dashboard app created successfully\n",
      "\n",
      "========================================\n",
      "📋 Test Summary:\n",
      "  ✅ PASS: Imports\n",
      "  ✅ PASS: Registry\n",
      "  ✅ PASS: Metric Calculation\n",
      "  ✅ PASS: API Creation\n",
      "  ✅ PASS: Dashboard Creation\n",
      "\n",
      "🎯 Overall: 5/5 tests passed\n",
      "\n",
      "🎉 All tests passed! Metrics Hub is ready to use.\n",
      "\n",
      "🚀 Next steps:\n",
      "  1. Run: python -c \"from metrics_hub.scaffolding.cli import cli; cli()\" list\n",
      "  2. Run: python -c \"from metrics_hub.scaffolding.cli import cli; cli()\" dashboard\n",
      "  3. Try the Jupyter notebook: Metrics_Hub_Demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Install the package from local directory\n",
    "# Run this from the root directory containing pyproject.toml\n",
    "!pip3 install -e \".[all]\"\n",
    "\n",
    "# Install additional Jupyter dependencies for Plotly visualization\n",
    "!pip3 install nbformat>=4.2.0 ipywidgets\n",
    "\n",
    "# 🧪 IMPORTANT: Test installation first!\n",
    "# The test_installation.py script verifies that everything works correctly\n",
    "print(\"🔍 Running installation verification test...\")\n",
    "!python3 test_installation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 About the Installation Test Script\n",
    "\n",
    "### 🎯 Purpose of `test_installation.py`\n",
    "\n",
    "The `test_installation.py` script is a **comprehensive verification tool** that ensures all Metrics Hub components are working correctly after installation. Here's what it does:\n",
    "\n",
    "#### ✅ **5 Critical Tests**:\n",
    "\n",
    "1. **🔍 Import Test**: Verifies all Python modules can be imported correctly\n",
    "   - Tests core functions (`register_metric`, `call_metric`, etc.)\n",
    "   - Tests API components (`create_api_app`)\n",
    "   - Tests dashboard components (`create_dashboard_app`)\n",
    "   - Tests registry system (`get_registry`)\n",
    "\n",
    "2. **📊 Registry Test**: Ensures the metric discovery system works\n",
    "   - Loads the metric registry\n",
    "   - Discovers available metrics\n",
    "   - Validates metric configurations\n",
    "\n",
    "3. **🧮 Calculation Test**: Verifies metrics can actually run\n",
    "   - Calls the example financial analysis metric\n",
    "   - Tests with real parameters\n",
    "   - Validates rich output types (DataFrames, Plotly charts)\n",
    "\n",
    "4. **🌐 API Test**: Confirms the FastAPI server can be created\n",
    "   - Creates the API application\n",
    "   - Ensures all endpoints are available\n",
    "\n",
    "5. **📊 Dashboard Test**: Validates the Dash dashboard works\n",
    "   - Creates the dashboard application\n",
    "   - Ensures interactive components are ready\n",
    "\n",
    "#### 🚨 **Why This Matters**:\n",
    "\n",
    "- **Early Problem Detection**: Catches issues before you start the demo\n",
    "- **Environment Validation**: Ensures your Python environment is compatible\n",
    "- **Dependency Verification**: Confirms all required packages are installed\n",
    "- **Quick Diagnostics**: Provides clear pass/fail results with error details\n",
    "\n",
    "#### 🔧 **When to Use**:\n",
    "\n",
    "- **After installation**: Always run this first\n",
    "- **Before demos**: Ensure everything works before presenting\n",
    "- **Troubleshooting**: When things aren't working as expected\n",
    "- **Environment changes**: After updating Python or dependencies\n",
    "\n",
    "The script gives you **confidence** that Metrics Hub is ready to use! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components\n",
    "import metrics_hub\n",
    "from metrics_hub import register_metric, get_metric, list_metrics, call_metric\n",
    "from metrics_hub.api import create_api_app\n",
    "from metrics_hub.dashboard import create_dashboard_app\n",
    "\n",
    "# Check package info\n",
    "print(f\"📦 Package Version: {metrics_hub.__version__}\")\n",
    "print(f\"📍 Package Location: {metrics_hub.__file__}\")\n",
    "print(f\"🔧 Available Functions: {metrics_hub.__all__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Step 3: Test Existing Example Metric\n",
    "\n",
    "Let's test the example financial analysis metric that comes with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available metrics\n",
    "available_metrics = list_metrics()\n",
    "print(f\"📊 Available Metrics ({len(available_metrics)}):\")\n",
    "for metric in available_metrics:\n",
    "    print(f\"  • {metric['id']}: {metric.get('name', 'No name')}\")\n",
    "    print(f\"    Category: {metric.get('category', 'Unknown')}\")\n",
    "    print(f\"    Description: {metric.get('description', 'No description')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the example financial analysis metric\n",
    "if available_metrics:\n",
    "    # Use the first available metric (should be example_financial_analysis)\n",
    "    metric_id = available_metrics[0]['id']\n",
    "    print(f\"🧪 Testing metric: {metric_id}\")\n",
    "    \n",
    "    # Call the metric with default parameters\n",
    "    result = call_metric(metric_id, num_assets=3, time_periods=50, volatility=0.15)\n",
    "    \n",
    "    print(f\"✅ Metric calculation successful!\")\n",
    "    print(f\"📊 Result keys: {list(result.keys())}\")\n",
    "    \n",
    "    # Display each output type\n",
    "    for key, value in result.items():\n",
    "        print(f\"\\n🔍 {key}: {type(value).__name__}\")\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "        elif hasattr(value, '__len__') and not isinstance(value, str):\n",
    "            print(f\"  Length: {len(value)}\")\n",
    "else:\n",
    "    print(\"❌ No metrics found. Let's create one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 4: Explore Rich Outputs\n",
    "\n",
    "Let's examine the different types of rich outputs that Metrics Hub supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have results, let's explore them\n",
    "if 'result' in locals() and result:\n",
    "    \n",
    "    # 1. DataFrame Output\n",
    "    if 'price_data' in result:\n",
    "        print(\"📊 DATAFRAME OUTPUT:\")\n",
    "        price_df = result['price_data']\n",
    "        print(f\"Shape: {price_df.shape}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        display(price_df.head())\n",
    "        \n",
    "        print(\"\\nData types:\")\n",
    "        print(price_df.dtypes)\n",
    "    \n",
    "    # 2. Summary Statistics\n",
    "    if 'summary_metrics' in result:\n",
    "        print(\"\\n📋 SUMMARY METRICS:\")\n",
    "        import json\n",
    "        print(json.dumps(result['summary_metrics'], indent=2))\n",
    "    \n",
    "    # 3. Statistics Table\n",
    "    if 'statistics_table' in result:\n",
    "        print(\"\\n📈 STATISTICS TABLE:\")\n",
    "        display(result['statistics_table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Step 5: Display Interactive Visualizations\n",
    "\n",
    "Let's display the Plotly charts generated by our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Plotly charts if available\n",
    "if 'result' in locals() and result:\n",
    "    \n",
    "    # Performance Chart\n",
    "    if 'performance_chart' in result:\n",
    "        print(\"📈 INTERACTIVE PERFORMANCE CHART:\")\n",
    "        try:\n",
    "            result['performance_chart'].show()\n",
    "        except ValueError as e:\n",
    "            if \"nbformat\" in str(e):\n",
    "                print(\"⚠️  Plotly display requires nbformat. Install with: pip install nbformat>=4.2.0\")\n",
    "                print(\"📊 Chart created successfully (would display in proper Jupyter environment)\")\n",
    "                print(f\"📏 Chart data shape: {len(result['performance_chart'].data)} traces\")\n",
    "            else:\n",
    "                print(f\"❌ Error displaying chart: {e}\")\n",
    "    \n",
    "    # Correlation Heatmap\n",
    "    if 'correlation_heatmap' in result:\n",
    "        print(\"\\n🎯 CORRELATION HEATMAP:\")\n",
    "        try:\n",
    "            result['correlation_heatmap'].show()\n",
    "        except ValueError as e:\n",
    "            if \"nbformat\" in str(e):\n",
    "                print(\"⚠️  Plotly display requires nbformat. Install with: pip install nbformat>=4.2.0\")\n",
    "                print(\"📊 Heatmap created successfully (would display in proper Jupyter environment)\")\n",
    "                print(f\"📏 Heatmap data shape: {len(result['correlation_heatmap'].data)} traces\")\n",
    "            else:\n",
    "                print(f\"❌ Error displaying heatmap: {e}\")\n",
    "else:\n",
    "    print(\"❌ No results available. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Step 6: Create a New Metric\n",
    "\n",
    "Now let's create a new metric to demonstrate the scaffolding system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new metric using the CLI\n",
    "!metrics-hub create \"Simple Calculator\" \\\n",
    "    --category demo \\\n",
    "    --description \"Simple mathematical calculations with visualization\" \\\n",
    "    --template simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Step 7: Implement the New Metric\n",
    "\n",
    "Let's examine and then implement our newly created metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files were created\n",
    "!ls -la metrics_hub/metrics/demo_simple_calculator.*\n",
    "!ls -la tests/test_demo_simple_calculator.py\n",
    "!ls -la deploy_demo_simple_calculator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated metric file\n",
    "with open('metrics_hub/metrics/demo_simple_calculator.py', 'r') as f:\n",
    "    print(\"📄 Generated Metric Code:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's create a more interesting implementation\nenhanced_metric_code = '''\n\"\"\"\nSimple Calculator Metric with Rich Outputs\n\nDemonstrates basic mathematical operations with visualization.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom metrics_hub import register_metric\nfrom typing import Dict, Any, List\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@register_metric(\"demo_simple_calculator\")\ndef calculate_simple_calculator(\n    numbers: List[float] = [1, 2, 3, 4, 5],\n    operation: str = \"all\"\n) -> Dict[str, Any]:\n    \"\"\"\n    Simple mathematical calculations with rich outputs.\n    \n    Args:\n        numbers: List of numbers to perform calculations on\n        operation: Type of operation (\"all\", \"sum\", \"mean\", \"std\")\n    \n    Returns:\n        Dict with calculations table, visualization, and summary\n    \"\"\"\n    try:\n        logger.info(f\"Calculating with numbers: {numbers}, operation: {operation}\")\n        \n        # Input validation\n        if not numbers:\n            raise ValueError(\"Numbers list cannot be empty\")\n        \n        # Convert to numpy array for calculations\n        arr = np.array(numbers)\n        \n        # Perform calculations\n        calculations = {\n            'sum': float(np.sum(arr)),\n            'mean': float(np.mean(arr)),\n            'median': float(np.median(arr)),\n            'std': float(np.std(arr)),\n            'min': float(np.min(arr)),\n            'max': float(np.max(arr)),\n            'count': len(arr)\n        }\n        \n        # Create results DataFrame\n        results_df = pd.DataFrame({\n            'Index': range(len(numbers)),\n            'Value': numbers,\n            'Squared': [x**2 for x in numbers],\n            'Cumulative_Sum': np.cumsum(numbers)\n        })\n        \n        # Create visualization\n        fig = go.Figure()\n        \n        # Add original values\n        fig.add_trace(go.Scatter(\n            x=results_df['Index'],\n            y=results_df['Value'],\n            mode='lines+markers',\n            name='Original Values',\n            line=dict(color='blue', width=3)\n        ))\n        \n        # Add mean line\n        fig.add_hline(\n            y=calculations['mean'],\n            line_dash=\"dash\",\n            line_color=\"red\",\n            annotation_text=f\"Mean: {calculations['mean']:.2f}\"\n        )\n        \n        fig.update_layout(\n            title='Number Sequence Analysis',\n            xaxis_title='Index',\n            yaxis_title='Value',\n            template='plotly_white',\n            height=400\n        )\n        \n        # Create summary based on operation\n        if operation == \"all\":\n            summary = calculations\n        else:\n            summary = {operation: calculations.get(operation, \"Invalid operation\")}\n        \n        result = {\n            'calculations_table': results_df,\n            'visualization': fig,\n            'summary_stats': summary\n        }\n        \n        logger.info(\"Calculation completed successfully\")\n        return result\n        \n    except Exception as e:\n        logger.error(f\"Calculation failed: {e}\")\n        raise RuntimeError(f\"Calculator metric failed: {e}\") from e\n'''\n\n# Write the enhanced metric code\nwith open('metrics_hub/metrics/demo_simple_calculator.py', 'w') as f:\n    f.write(enhanced_metric_code)\n\nprint(\"✅ Enhanced metric implementation written!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Step 8: Test the New Metric\n",
    "\n",
    "Let's reload the metrics and test our new implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel or reimport to pick up changes\n",
    "import importlib\n",
    "import metrics_hub.core\n",
    "importlib.reload(metrics_hub.core)\n",
    "\n",
    "# Force reload the registry\n",
    "from metrics_hub.core import get_registry\n",
    "registry = get_registry()\n",
    "registry.load_metrics()  # Reload metrics\n",
    "\n",
    "# List metrics again\n",
    "updated_metrics = list_metrics()\n",
    "print(f\"📊 Updated Metrics ({len(updated_metrics)}):\")\n",
    "for metric in updated_metrics:\n",
    "    print(f\"  • {metric['id']}: {metric.get('name', 'No name')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our new calculator metric\n",
    "try:\n",
    "    calc_result = call_metric(\n",
    "        \"demo_simple_calculator\",\n",
    "        numbers=[10, 20, 15, 25, 30, 18, 22],\n",
    "        operation=\"all\"\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Calculator metric test successful!\")\n",
    "    print(f\"📊 Result keys: {list(calc_result.keys())}\")\n",
    "    \n",
    "    # Display results\n",
    "    if 'calculations_table' in calc_result:\n",
    "        print(\"\\n📊 Calculations Table:\")\n",
    "        display(calc_result['calculations_table'])\n",
    "    \n",
    "    if 'summary_stats' in calc_result:\n",
    "        print(\"\\n📋 Summary Statistics:\")\n",
    "        import json\n",
    "        print(json.dumps(calc_result['summary_stats'], indent=2))\n",
    "    \n",
    "    if 'visualization' in calc_result:\n",
    "        print(\"\\n📈 Visualization:\")\n",
    "        calc_result['visualization'].show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing calculator metric: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗑️ Step 9: Metric Cleanup and Removal\n",
    "\n",
    "Metrics Hub includes a convenient cleanup function to remove metrics and all their related files when they're no longer needed.\n",
    "\n",
    "### 🎯 Remove Command Features\n",
    "\n",
    "The `metrics-hub remove` command cleans up all files associated with a metric:\n",
    "- ✅ **Python implementation** (`metrics_hub/metrics/{metric_id}.py`)\n",
    "- ✅ **YAML configuration** (`metrics_hub/metrics/{metric_id}.yaml`) \n",
    "- ✅ **Test file** (`tests/test_{metric_id}.py`)\n",
    "- ✅ **Deployment script** (`deploy_{metric_id}.py`)\n",
    "- ✅ **Requirements file** (if exists)\n",
    "\n",
    "### 📋 Safety Features\n",
    "\n",
    "- **Existence check**: Verifies metric exists before removal\n",
    "- **File listing**: Shows exactly what will be removed\n",
    "- **Confirmation prompt**: Requires user confirmation (unless `--force` used)\n",
    "- **Error handling**: Reports any files that couldn't be removed\n",
    "- **Summary report**: Shows removal results\n",
    "\n",
    "### 🔧 Command Usage\n",
    "\n",
    "```bash\n",
    "# Remove a metric (with confirmation)\n",
    "metrics-hub remove metric_id\n",
    "\n",
    "# Remove a metric without confirmation\n",
    "metrics-hub remove metric_id --force\n",
    "\n",
    "# See help\n",
    "metrics-hub remove --help\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a temporary test metric to demonstrate removal\n",
    "print(\"🛠️ Creating a temporary metric for removal demonstration...\")\n",
    "\n",
    "!metrics-hub create \"Temp Test Metric\" \\\n",
    "    --category temp \\\n",
    "    --description \"Temporary metric for removal demo\" \\\n",
    "    --template simple\n",
    "\n",
    "print(\"\\n📋 Let's see what files were created:\")\n",
    "!ls -la metrics_hub/metrics/temp_temp_test_metric.*\n",
    "!ls -la tests/test_temp_temp_test_metric.py 2>/dev/null || echo \"Test file: Not found\"\n",
    "!ls -la deploy_temp_temp_test_metric.py 2>/dev/null || echo \"Deploy file: Not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's demonstrate the remove command\n",
    "print(\"🗑️ Demonstrating metric removal...\")\n",
    "\n",
    "# First, let's see the help for the remove command\n",
    "print(\"📖 Remove command help:\")\n",
    "!metrics-hub remove --help\n",
    "\n",
    "print(\"\\n📊 Current metrics before removal:\")\n",
    "!metrics-hub list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the temporary metric using --force to skip confirmation in the notebook\n",
    "print(\"🗑️ Removing the temporary metric...\")\n",
    "\n",
    "!metrics-hub remove temp_temp_test_metric --force\n",
    "\n",
    "print(\"\\n📊 Verify the metric was removed:\")\n",
    "!metrics-hub list\n",
    "\n",
    "print(\"\\n📋 Check if files were actually removed:\")\n",
    "!ls -la metrics_hub/metrics/temp_temp_test_metric.* 2>/dev/null || echo \"✅ Metric files removed\"\n",
    "!ls -la tests/test_temp_temp_test_metric.py 2>/dev/null || echo \"✅ Test file removed\"  \n",
    "!ls -la deploy_temp_temp_test_metric.py 2>/dev/null || echo \"✅ Deploy file removed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Step 10: API Server Demo\n",
    "\n",
    "Let's demonstrate the FastAPI server capabilities (this will run in the background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the FastAPI app and ensure metrics are loaded\nfrom metrics_hub.api import create_api_app\nimport uvicorn\nimport threading\nimport time\nimport requests\n\n# Force reload metrics to ensure demo_simple_calculator is available\nimport importlib\nimport metrics_hub.core\ntry:\n    import metrics_hub.metrics.demo_simple_calculator  # Import the calculator metric\nexcept ImportError:\n    print(\"⚠️  demo_simple_calculator not found, creating it...\")\n    !metrics-hub create \"Simple Calculator\" --category demo --description \"Simple mathematical calculations\" --template simple\n\n# Reload the core module and registry\nimportlib.reload(metrics_hub.core)\nfrom metrics_hub.core import get_registry\nregistry = get_registry()\nregistry.load_metrics()\n\n# Verify metrics are available\nfrom metrics_hub import list_metrics\navailable_metrics = list_metrics()\nprint(f\"📊 Available metrics for API testing: {[m['id'] for m in available_metrics]}\")\n\n# Create the FastAPI app\napp = create_api_app()\n\n# Function to run the server in background\ndef run_server():\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"warning\")\n\n# Start server in background thread\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\n# Wait for server to start\nprint(\"🚀 Starting API server...\")\ntime.sleep(3)\n\n# Test the server\ntry:\n    response = requests.get(\"http://localhost:8000/\")\n    print(f\"✅ API Server is running! Status: {response.status_code}\")\n    print(f\"📊 Response: {response.json()}\")\nexcept Exception as e:\n    print(f\"❌ Server not responding: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test API endpoints\nbase_url = \"http://localhost:8000\"\n\ntry:\n    # List metrics via API\n    print(\"📋 Testing /metrics endpoint:\")\n    metrics_response = requests.get(f\"{base_url}/metrics\")\n    if metrics_response.status_code == 200:\n        api_metrics = metrics_response.json()\n        print(f\"Found {len(api_metrics)} metrics via API\")\n        metric_ids = [metric['id'] for metric in api_metrics]\n        for metric in api_metrics:\n            print(f\"  • {metric['id']}\")\n    else:\n        print(f\"❌ Metrics endpoint failed with status: {metrics_response.status_code}\")\n        metric_ids = []\n    \n    # Test metric calculation via API (only if metric exists)\n    if \"demo_simple_calculator\" in metric_ids:\n        print(\"\\n🧮 Testing metric calculation via API:\")\n        calc_payload = {\n            \"metric_id\": \"demo_simple_calculator\",\n            \"inputs\": {\n                \"numbers\": [5, 10, 15, 20],\n                \"operation\": \"all\"\n            }\n        }\n        \n        calc_response = requests.post(f\"{base_url}/calculate\", json=calc_payload)\n        if calc_response.status_code == 200:\n            api_result = calc_response.json()\n            print(f\"✅ Calculation successful!\")\n            print(f\"📊 Result type: {api_result.get('result_type', 'unknown')}\")\n            print(f\"📊 Success: {api_result.get('success', False)}\")\n            \n            # Show summary stats from API result (safer handling)\n            if 'result' in api_result and api_result['result'] and 'summary_stats' in api_result['result']:\n                print(\"\\n📋 Summary from API:\")\n                import json\n                print(json.dumps(api_result['result']['summary_stats'], indent=2))\n        else:\n            print(f\"❌ API calculation failed with status: {calc_response.status_code}\")\n            print(f\"Response: {calc_response.text}\")\n    else:\n        print(\"\\n⚠️  demo_simple_calculator metric not found, skipping calculation test\")\n        print(\"Available metrics:\", metric_ids)\n    \n    # Test HTML endpoint (use first available metric)\n    if metric_ids:\n        test_metric = \"demo_simple_calculator\" if \"demo_simple_calculator\" in metric_ids else metric_ids[0]\n        print(f\"\\n🌐 Testing HTML endpoint with metric '{test_metric}':\")\n        \n        if test_metric == \"demo_simple_calculator\":\n            html_url = f\"{base_url}/calculate/demo_simple_calculator/html?numbers=[1,2,3,4,5]&operation=all\"\n        else:\n            # Use default parameters for other metrics\n            html_url = f\"{base_url}/calculate/{test_metric}/html?num_assets=2&time_periods=10\"\n        \n        html_response = requests.get(html_url)\n        if html_response.status_code == 200:\n            response_text = html_response.text if hasattr(html_response, 'text') and html_response.text else \"\"\n            print(f\"✅ HTML endpoint working! Content length: {len(response_text)}\")\n            print(f\"🌐 URL: {html_url}\")\n            print(\"   (You can open this URL in a browser to see the full HTML report)\")\n        else:\n            print(f\"❌ HTML endpoint failed with status: {html_response.status_code}\")\n            print(f\"Response: {html_response.text[:200]}\")\n    else:\n        print(\"\\n⚠️  No metrics available for HTML testing\")\n    \nexcept Exception as e:\n    print(f\"❌ API testing failed: {e}\")\n    import traceback\n    print(\"Full error details:\")\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 11: Dashboard Demo Setup\n",
    "\n",
    "Now let's set up the Dash dashboard. Note: The dashboard will run in a separate process and open in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dashboard app created successfully!\n",
      "\n",
      "🎯 Dashboard Features:\n",
      "  • Dynamic form generation based on metric inputs\n",
      "  • Real-time metric calculation\n",
      "  • Rich output display (tables, charts, statistics)\n",
      "  • Interactive Plotly visualizations\n",
      "  • Error handling and debugging\n",
      "\n",
      "📝 To run the dashboard:\n",
      "1. In a terminal, navigate to this directory\n",
      "2. Run: metrics-hub dashboard\n",
      "3. Open http://localhost:8050 in your browser\n",
      "\n",
      "Or run the cell below to start it programmatically.\n"
     ]
    }
   ],
   "source": [
    "# Create dashboard app\n",
    "from metrics_hub.dashboard import create_dashboard_app\n",
    "import webbrowser\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "# Create the dashboard app\n",
    "dashboard_app = create_dashboard_app()\n",
    "\n",
    "print(\"📊 Dashboard app created successfully!\")\n",
    "print(\"\\n🎯 Dashboard Features:\")\n",
    "print(\"  • Dynamic form generation based on metric inputs\")\n",
    "print(\"  • Real-time metric calculation\")\n",
    "print(\"  • Rich output display (tables, charts, statistics)\")\n",
    "print(\"  • Interactive Plotly visualizations\")\n",
    "print(\"  • Error handling and debugging\")\n",
    "\n",
    "print(\"\\n📝 To run the dashboard:\")\n",
    "print(\"1. In a terminal, navigate to this directory\")\n",
    "print(\"2. Run: metrics-hub dashboard\")\n",
    "print(\"3. Open http://localhost:8050 in your browser\")\n",
    "print(\"\\nOr run the cell below to start it programmatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dashboard demo function ready.\n",
      "💡 Uncomment the last line in run_dashboard_demo() to start the dashboard.\n",
      "⚠️  Note: Running the dashboard will block this notebook until stopped.\n",
      "\n",
      "🎯 RECOMMENDED: Run dashboard in terminal for best experience:\n",
      "   1. Open a new terminal\n",
      "   2. Navigate to this project directory\n",
      "   3. Run: metrics-hub dashboard\n",
      "   4. Open http://localhost:8050 in your browser\n"
     ]
    }
   ],
   "source": [
    "# Function to run dashboard (uncomment to run)\n",
    "# WARNING: This will start a dashboard server that runs until the kernel is restarted\n",
    "\n",
    "def run_dashboard_demo():\n",
    "    \"\"\"Run dashboard in demo mode.\"\"\"\n",
    "    from metrics_hub.dashboard import run_dashboard\n",
    "    print(\"🚀 Starting Metrics Hub Dashboard...\")\n",
    "    print(\"🌐 URL: http://localhost:8050\")\n",
    "    print(\"\\n📊 In the dashboard, you can:\")\n",
    "    print(\"  1. Select a metric from the dropdown\")\n",
    "    print(\"  2. Fill in the input parameters\")\n",
    "    print(\"  3. Click 'Calculate' to see results\")\n",
    "    print(\"  4. View tables, charts, and statistics\")\n",
    "    print(\"\\n⚠️  To stop the dashboard, interrupt the kernel\")\n",
    "    \n",
    "    # Uncomment the next line to actually run the dashboard\n",
    "    # run_dashboard(host=\"127.0.0.1\", port=8050, debug=True)\n",
    "\n",
    "print(\"📊 Dashboard demo function ready.\")\n",
    "print(\"💡 Uncomment the last line in run_dashboard_demo() to start the dashboard.\")\n",
    "print(\"⚠️  Note: Running the dashboard will block this notebook until stopped.\")\n",
    "\n",
    "# Show instructions instead of running\n",
    "print(\"\\n🎯 RECOMMENDED: Run dashboard in terminal for best experience:\")\n",
    "print(\"   1. Open a new terminal\")\n",
    "print(\"   2. Navigate to this project directory\")\n",
    "print(\"   3. Run: metrics-hub dashboard\")\n",
    "print(\"   4. Open http://localhost:8050 in your browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Step 12: Deployment Preparation\n",
    "\n",
    "Let's examine the deployment script that was generated for Posit Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the deployment script\n",
    "deploy_file = 'deploy_demo_simple_calculator.py'\n",
    "if os.path.exists(deploy_file):\n",
    "    with open(deploy_file, 'r') as f:\n",
    "        deploy_content = f.read()\n",
    "    \n",
    "    print(\"📄 Generated Deployment Script:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(deploy_content)\n",
    "else:\n",
    "    print(f\"❌ Deployment file {deploy_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployment script locally\n",
    "print(\"🧪 Testing deployment script locally...\")\n",
    "print(\"\\n📝 In production, you would:\")\n",
    "print(\"1. Test locally: python deploy_demo_simple_calculator.py\")\n",
    "print(\"2. Deploy to Posit Connect:\")\n",
    "print(\"   rsconnect deploy fastapi deploy_demo_simple_calculator.py \\\\\")\n",
    "print(\"       --account myaccount \\\\\")\n",
    "print(\"       --title 'Simple Calculator API'\")\n",
    "\n",
    "print(\"\\n🌐 Deployed endpoints would include:\")\n",
    "print(\"  • /metrics - List all metrics\")\n",
    "print(\"  • /calculate - JSON calculation\")\n",
    "print(\"  • /calculate/demo_simple_calculator/html - HTML report\")\n",
    "print(\"  • /calculate/demo_simple_calculator/csv - CSV download\")\n",
    "print(\"  • /docs - Interactive API documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 13: Testing Framework Demo\n",
    "\n",
    "Let's run the generated tests to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the generated tests\n",
    "test_file = 'tests/test_demo_simple_calculator.py'\n",
    "if os.path.exists(test_file):\n",
    "    print(\"🧪 Running generated tests...\")\n",
    "    !python -m pytest {test_file} -v\n",
    "else:\n",
    "    print(f\"❌ Test file {test_file} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 14: Output Format Demonstrations\n",
    "\n",
    "Let's demonstrate the different output formats supported by Metrics Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate API output formats\n",
    "from metrics_hub.api import _process_result\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data\n",
    "sample_df = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C'],\n",
    "    'Sales': [100, 150, 120],\n",
    "    'Profit': [20, 35, 25]\n",
    "})\n",
    "\n",
    "sample_chart = go.Figure(data=[\n",
    "    go.Bar(x=sample_df['Product'], y=sample_df['Sales'], name='Sales')\n",
    "])\n",
    "sample_chart.update_layout(title='Sample Sales Chart')\n",
    "\n",
    "# Test different output formats\n",
    "print(\"📊 DATAFRAME OUTPUT FORMATS:\")\n",
    "print(\"\\n1. JSON format:\")\n",
    "result_type, processed = _process_result(sample_df, \"json\")\n",
    "print(f\"Type: {result_type}\")\n",
    "print(f\"Data: {processed[:200]}...\" if len(str(processed)) > 200 else f\"Data: {processed}\")\n",
    "\n",
    "print(\"\\n2. HTML format:\")\n",
    "result_type, processed = _process_result(sample_df, \"html\")\n",
    "print(f\"Type: {result_type}\")\n",
    "print(f\"HTML length: {len(processed)} characters\")\n",
    "\n",
    "print(\"\\n3. CSV format:\")\n",
    "result_type, processed = _process_result(sample_df, \"csv\")\n",
    "print(f\"Type: {result_type}\")\n",
    "print(f\"CSV data:\\n{processed}\")\n",
    "\n",
    "print(\"\\n📈 PLOTLY OUTPUT FORMATS:\")\n",
    "print(\"\\n1. JSON format:\")\n",
    "result_type, processed = _process_result(sample_chart, \"json\")\n",
    "print(f\"Type: {result_type}\")\n",
    "print(f\"Keys: {list(processed.keys()) if isinstance(processed, dict) else 'Not a dict'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 15: Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the full Metrics Hub demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"🎉 METRICS HUB DEMO COMPLETE!\\n\")\n",
    "\n",
    "print(\"✅ What we accomplished:\")\n",
    "print(\"  📦 Installed Metrics Hub from local source\")\n",
    "print(\"  🔍 Explored existing example metrics\")\n",
    "print(\"  🛠️ Created a new metric with scaffolding\")\n",
    "print(\"  📊 Implemented rich outputs (DataFrames, Plotly charts)\")\n",
    "print(\"  🧪 Tested metrics with various input parameters\")\n",
    "print(\"  🗑️ Demonstrated metric removal and cleanup\")\n",
    "print(\"  🌐 Demonstrated API server capabilities\")\n",
    "print(\"  📊 Set up interactive dashboard\")\n",
    "print(\"  🚀 Prepared deployment for Posit Connect\")\n",
    "print(\"  🧪 Ran automated tests\")\n",
    "print(\"  🎨 Explored multiple output formats\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"  1. Run the dashboard: metrics-hub dashboard\")\n",
    "print(\"  2. Create your own metrics with: metrics-hub create\")\n",
    "print(\"  3. Remove metrics when done: metrics-hub remove\")\n",
    "print(\"  4. Test in the interactive dashboard\")\n",
    "print(\"  5. Deploy to Posit Connect when ready\")\n",
    "\n",
    "print(\"\\n📚 Resources:\")\n",
    "print(\"  • User Guide: USER_GUIDE.md\")\n",
    "print(\"  • Architecture: ARCHITECTURE.md\")\n",
    "print(\"  • Implementation: IMPLEMENTATION_GUIDE.md\")\n",
    "\n",
    "print(\"\\n🔧 Available Commands:\")\n",
    "!metrics-hub --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Interactive Demo Instructions\n",
    "\n",
    "### 🌐 Try the Dashboard (Recommended)\n",
    "\n",
    "1. **Open a terminal** and navigate to this project directory\n",
    "2. **Run the dashboard**: `metrics-hub dashboard`\n",
    "3. **Open your browser** to http://localhost:8050\n",
    "4. **Select a metric** from the dropdown menu\n",
    "5. **Enter parameters** in the auto-generated form\n",
    "6. **Click Calculate** to see rich outputs\n",
    "7. **Explore** the interactive charts and tables\n",
    "\n",
    "### 🌐 Try the API\n",
    "\n",
    "1. **Open another terminal** and run: `metrics-hub api`\n",
    "2. **Visit** http://localhost:8000/docs for interactive API documentation\n",
    "3. **Try different endpoints**:\n",
    "   - GET /metrics (list all metrics)\n",
    "   - POST /calculate (calculate with JSON)\n",
    "   - GET /calculate/{metric_id}/html (rich HTML report)\n",
    "   - GET /calculate/{metric_id}/csv (CSV download)\n",
    "\n",
    "### 🛠️ Create Your Own Metrics\n",
    "\n",
    "1. **Generate new metric**: `metrics-hub create \"Your Metric Name\" --template dataframe`\n",
    "2. **Edit the implementation** in `metrics_hub/metrics/`\n",
    "3. **Test in dashboard** or with `call_metric()`\n",
    "4. **Remove when done**: `metrics-hub remove your_metric_id`\n",
    "5. **Deploy** when ready with the generated deployment script\n",
    "\n",
    "## 🆘 Troubleshooting\n",
    "\n",
    "### Quick Verification\n",
    "```bash\n",
    "# Test that everything works\n",
    "python test_installation.py\n",
    "\n",
    "# List available metrics\n",
    "metrics-hub list\n",
    "\n",
    "# Create test metric\n",
    "metrics-hub create \"Test Metric\" --template simple\n",
    "\n",
    "# Remove test metric\n",
    "metrics-hub remove test_metric_id --force\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### **Plotly Charts Not Displaying**\n",
    "```bash\n",
    "# Install Jupyter dependencies for Plotly\n",
    "pip install nbformat>=4.2.0 ipywidgets\n",
    "\n",
    "# Alternative: View charts in dashboard or API HTML endpoints\n",
    "metrics-hub dashboard\n",
    "```\n",
    "\n",
    "#### **Other Common Issues**\n",
    "- **Import Errors**: Ensure package installed with `pip install -e \".[all]\"`\n",
    "- **Python Version**: Requires Python 3.11+\n",
    "- **Missing Dependencies**: Install with all features using `[all]` option\n",
    "- **Port Conflicts**: Dashboard uses 8050, API uses 8000\n",
    "\n",
    "### Getting Help\n",
    "- **Documentation**: README.md, USER_GUIDE.md, ARCHITECTURE.md\n",
    "- **Test Script**: Run `python test_installation.py` for diagnostics\n",
    "- **Example Code**: Check `metrics_hub/metrics/example_financial_analysis.py`\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "### Documentation Files\n",
    "- **📖 README.md**: Complete package overview with features\n",
    "- **👤 USER_GUIDE.md**: Comprehensive user documentation  \n",
    "- **🏗️ ARCHITECTURE.md**: Technical architecture and design\n",
    "- **🔧 IMPLEMENTATION_GUIDE.md**: Developer implementation details\n",
    "\n",
    "### Key Commands Reference\n",
    "```bash\n",
    "# Installation and verification\n",
    "pip install -e \".[all]\"\n",
    "pip install nbformat>=4.2.0 ipywidgets  # For Jupyter Plotly support\n",
    "python test_installation.py\n",
    "\n",
    "# CLI commands - simplified syntax\n",
    "metrics-hub --help\n",
    "metrics-hub list\n",
    "metrics-hub create \"Metric Name\" --template dataframe\n",
    "metrics-hub remove metric_id\n",
    "metrics-hub dashboard\n",
    "metrics-hub api\n",
    "\n",
    "# Testing\n",
    "pytest tests/\n",
    "python test_installation.py\n",
    "```\n",
    "\n",
    "### Production Deployment\n",
    "```bash\n",
    "# Test deployment locally\n",
    "python deploy_your_metric.py\n",
    "\n",
    "# Deploy to Posit Connect\n",
    "rsconnect deploy fastapi deploy_your_metric.py \\\n",
    "    --account myaccount \\\n",
    "    --title \"Your Metric API\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've completed the **Metrics Hub Demo** and are now ready to:\n",
    "\n",
    "✅ **Build sophisticated metrics** with rich outputs (DataFrames, Plotly charts)  \n",
    "✅ **Test interactively** using the built-in dashboard  \n",
    "✅ **Remove metrics easily** when no longer needed  \n",
    "✅ **Deploy to production** with Posit Connect integration  \n",
    "✅ **Create scalable analytics** with the scaffolding system  \n",
    "\n",
    "**🚀 Happy metric building with Metrics Hub! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}